Chunk parsed string into 500 token bits

Done use langchain text splitter to split into chunks

send parsed strings to be embedded by api
Parsing done but need to set up database before creating embeddings

set up fastAPI to be always connected to zilliz/milvus when app is running
when app shuts down then disconnect from server

Milvus database
create milvus connection to docker db 
create a different DB based on user signed in (Just for Dev)
create schema of pdf data for collections
collection for each different pdf 
save embeddings in vector database

Plan
Upon user sign in to dashboard create connection to database
if first sign in then create database
if previous sign in connect to existing database

Upon user selecting a pdf to chat to
Connect to the specific pdf collection 
When user ask question to chatbot query collection to give better response

if user upload pdf
create new collection with embeddings from pdf
go to flow of user ask question to chatbot 

User asks question query the collection related to the pdf

Save number of prompt tokens and completitions token 
into database when using regular openAI API